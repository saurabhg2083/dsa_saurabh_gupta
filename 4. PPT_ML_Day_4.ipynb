{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09791d09",
   "metadata": {},
   "source": [
    "#### General Linear Model:\n",
    "\n",
    "    1. What is the purpose of the General Linear Model (GLM)?\n",
    "    2. What are the key assumptions of the General Linear Model?\n",
    "    3. How do you interpret the coefficients in a GLM?\n",
    "    4. What is the difference between a univariate and multivariate GLM?\n",
    "    5. Explain the concept of interaction effects in a GLM.\n",
    "    6. How do you handle categorical predictors in a GLM?\n",
    "    7. What is the purpose of the design matrix in a GLM?\n",
    "    8. How do you test the significance of predictors in a GLM?\n",
    "    9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "    10. Explain the concept of deviance in a GLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6922da7",
   "metadata": {},
   "source": [
    "1. The General Linear Model (GLM) is a statistical framework used to model the relationship between a dependent variable and one or more independent variables. It provides a flexible approach to analyze and understand the relationships between variables, making it widely used in various fields such as regression analysis, analysis of variance (ANOVA), and analysis of covariance (ANCOVA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eecb4e",
   "metadata": {},
   "source": [
    "2. The General Linear Model (GLM) makes several assumptions about the data in order to ensure the validity and accuracy of the model's estimates and statistical inferences. These assumptions are important to consider when applying the GLM to a dataset. Here are the key assumptions of the GLM:\n",
    "\n",
    "    1. Linearity: The GLM assumes that the relationship between the dependent variable and the independent variables is linear. This means that the effect of each independent variable on the dependent variable is additive and constant across the range of the independent variables.\n",
    "\n",
    "    2. Independence: The observations or cases in the dataset should be independent of each other. This assumption implies that there is no systematic relationship or dependency between observations. Violations of this assumption, such as autocorrelation in time series data or clustered observations, can lead to biased and inefficient parameter estimates.\n",
    "\n",
    "    3. Homoscedasticity: Homoscedasticity assumes that the variance of the errors (residuals) is constant across all levels of the independent variables. In other words, the spread of the residuals should be consistent throughout the range of the predictors. Heteroscedasticity, where the variance of the errors varies with the levels of the predictors, violates this assumption and can impact the validity of statistical tests and confidence intervals.\n",
    "\n",
    "    4. Normality: The GLM assumes that the errors or residuals follow a normal distribution. This assumption is necessary for valid hypothesis testing, confidence intervals, and model inference. Violations of normality can affect the accuracy of parameter estimates and hypothesis tests.\n",
    "\n",
    "    5. No Multicollinearity: Multicollinearity refers to a high degree of correlation between independent variables in the model. The GLM assumes that the independent variables are not perfectly correlated with each other, as this can lead to instability and difficulty in estimating the individual effects of the predictors.\n",
    "    \n",
    "    6. No Endogeneity: Endogeneity occurs when there is a correlation between the error term and one or more independent variables. This violates the assumption that the errors are independent of the predictors and can lead to biased and inconsistent parameter estimates.\n",
    "\n",
    "    7. Correct Specification: The GLM assumes that the model is correctly specified, meaning that the functional form of the relationship between the variables is accurately represented in the model. Omitting relevant variables or including irrelevant variables can lead to biased estimates and incorrect inferences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267664ed",
   "metadata": {},
   "source": [
    "3. Interpreting the coefficients in the General Linear Model (GLM) allows us to understand the relationships between the independent variables and the dependent variable. The coefficients provide information about the magnitude and direction of the effect that each independent variable has on the dependent variable, assuming all other variables in the model are held constant. Here's how you can interpret the coefficients in the GLM:\n",
    "\n",
    "    1. Coefficient Sign:\n",
    "    The sign (+ or -) of the coefficient indicates the direction of the relationship between the independent variable and the dependent variable. A positive coefficient indicates a positive relationship, meaning that an increase in the independent variable is associated with an increase in the dependent variable. Conversely, a negative coefficient indicates a negative relationship, where an increase in the independent variable is associated with a decrease in the dependent variable.\n",
    "\n",
    "    2. Magnitude:\n",
    "    The magnitude of the coefficient reflects the size of the effect that the independent variable has on the dependent variable, all else being equal. Larger coefficient values indicate a stronger influence of the independent variable on the dependent variable. For example, if the coefficient for a variable is 0.5, it means that a one-unit increase in the independent variable is associated with a 0.5-unit increase (or decrease, depending on the sign) in the dependent variable.\n",
    "\n",
    "    3. Statistical Significance:\n",
    "    The statistical significance of a coefficient is determined by its p-value. A low p-value (typically less than 0.05) suggests that the coefficient is statistically significant, indicating that the relationship between the independent variable and the dependent variable is unlikely to occur by chance. On the other hand, a high p-value suggests that the coefficient is not statistically significant, meaning that the relationship may not be reliable.\n",
    "\n",
    "    4. Adjusted vs. Unadjusted Coefficients:\n",
    "    In some cases, models with multiple independent variables may include adjusted coefficients. These coefficients take into account the effects of other variables in the model. Adjusted coefficients provide a more accurate estimate of the relationship between a specific independent variable and the dependent variable, considering the influences of other predictors.\n",
    "\n",
    "    It's important to note that interpretation of coefficients should consider the specific context and units of measurement for the variables involved. Additionally, the interpretation becomes more complex when dealing with categorical variables, interaction terms, or transformations of variables. In such cases, it's important to interpret the coefficients relative to the reference category or in the context of the specific interaction or transformation being modeled.\n",
    "\n",
    "    Overall, interpreting coefficients in the GLM helps us understand the relationships between variables and provides valuable insights into the factors that influence the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80647187",
   "metadata": {},
   "source": [
    "4. \n",
    "Simple:\n",
    "    Simple linear regression involves a single independent variable (X) and a continuous dependent variable (Y). It models the relationship between X and Y as a straight line. For example, consider a dataset that contains information about students' study hours (X) and their corresponding exam scores (Y). Simple linear regression can be used to model how study hours impact exam scores and make predictions about the expected score for a given number of study hours.\n",
    "\n",
    "Multiple:\n",
    "    Multiple linear regression involves two or more independent variables (X1, X2, X3, etc.) and a continuous dependent variable (Y). It models the relationship between the independent variables and the dependent variable. For instance, imagine a dataset that includes information about a car's price (Y) based on its attributes such as mileage (X1), engine size (X2), and age (X3). Multiple linear regression can be used to analyze how these factors influence the price of a car and make price predictions for new cars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe25e48",
   "metadata": {},
   "source": [
    "5. \n",
    "The design matrix, also known as the model matrix or feature matrix, is a crucial component of the General Linear Model (GLM). It is a structured representation of the independent variables in the GLM, organized in a matrix format. The design matrix serves the purpose of encoding the relationships between the independent variables and the dependent variable, allowing the GLM to estimate the coefficients and make predictions. Here's the purpose of the design matrix in the GLM:\n",
    "\n",
    "    1. Encoding Independent Variables:\n",
    "    The design matrix represents the independent variables in a structured manner. Each column of the matrix corresponds to a specific independent variable, and each row corresponds to an observation or data point. The design matrix encodes the values of the independent variables for each observation, allowing the GLM to incorporate them into the model.\n",
    "\n",
    "    2. Incorporating Nonlinear Relationships:\n",
    "    The design matrix can include transformations or interactions of the original independent variables to capture nonlinear relationships between the predictors and the dependent variable. For example, polynomial terms, logarithmic transformations, or interaction terms can be included in the design matrix to account for nonlinearities or interactions in the GLM.\n",
    "\n",
    "    3. Handling Categorical Variables:\n",
    "    Categorical variables need to be properly encoded to be included in the GLM. The design matrix can handle categorical variables by using dummy coding or other encoding schemes. Dummy variables are binary variables representing the categories of the original variable. By encoding categorical variables appropriately in the design matrix, the GLM can incorporate them in the model and estimate the corresponding coefficients.\n",
    "\n",
    "    4. Estimating Coefficients:\n",
    "    The design matrix allows the GLM to estimate the coefficients for each independent variable. By incorporating the design matrix into the GLM's estimation procedure, the model determines the relationship between the independent variables and the dependent variable, estimating the magnitude and significance of the effects of each predictor.\n",
    "\n",
    "    5. Making Predictions:\n",
    "    Once the GLM estimates the coefficients, the design matrix is used to make predictions for new, unseen data points. By multiplying the design matrix of the new data with the estimated coefficients, the GLM can generate predictions for the dependent variable based on the values of the independent variables.\n",
    "\n",
    "Here's an example to illustrate the purpose of the design matrix:\n",
    "\n",
    "Suppose we have a GLM with a continuous dependent variable (Y) and two independent variables (X1 and X2). The design matrix would have three columns: one for the intercept (usually a column of ones), one for X1, and one for X2. Each row in the design matrix represents an observation, and the values in the corresponding columns represent the values of X1 and X2 for that observation. The design matrix allows the GLM to estimate the coefficients for X1 and X2, capturing the relationship between the independent variables and the dependent variable.\n",
    "\n",
    "In summary, the design matrix plays a crucial role in the GLM by encoding the independent variables, enabling the estimation of coefficients, and facilitating predictions. It provides a structured representation of the independent variables that can handle nonlinearities, interactions, and categorical variables, allowing the GLM to capture the relationships between the predictors and the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a61d739",
   "metadata": {},
   "source": [
    "6. \n",
    "\n",
    "Handling categorical variables in the General Linear Model (GLM) requires appropriate encoding techniques to incorporate them into the model effectively. Categorical variables represent qualitative attributes and can significantly impact the relationship with the dependent variable. Here are a few common methods for handling categorical variables in the GLM:\n",
    "\n",
    "1. Dummy Coding (Binary Encoding):\n",
    "Dummy coding, also known as binary encoding, is a widely used technique to handle categorical variables in the GLM. It involves creating binary (0/1) dummy variables for each category within the categorical variable. The reference category is represented by 0 values for all dummy variables, while the other categories are encoded with 1 for the corresponding dummy variable.\n",
    "\n",
    "Example:\n",
    "Suppose we have a categorical variable \"Color\" with three categories: Red, Green, and Blue. We create two dummy variables: \"Green\" and \"Blue.\" The reference category (Red) will have 0 values for both dummy variables. If an observation has the category \"Green,\" the \"Green\" dummy variable will have a value of 1, while the \"Blue\" dummy variable will be 0.\n",
    "\n",
    "2. Effect Coding (Deviation Encoding):\n",
    "Effect coding, also called deviation coding, is another encoding technique for categorical variables in the GLM. In effect coding, each category is represented by a dummy variable, similar to dummy coding. However, unlike dummy coding, the reference category has -1 values for the corresponding dummy variable, while the other categories have 0 or 1 values.\n",
    "\n",
    "Example:\n",
    "Continuing with the \"Color\" categorical variable example, the reference category (Red) will have -1 values for both dummy variables. The \"Green\" category will have a value of 1 for the \"Green\" dummy variable and 0 for the \"Blue\" dummy variable. The \"Blue\" category will have a value of 0 for the \"Green\" dummy variable and 1 for the \"Blue\" dummy variable.\n",
    "\n",
    "3. One-Hot Encoding:\n",
    "One-hot encoding is another popular technique for handling categorical variables. It creates a separate binary variable for each category within the categorical variable. Each variable represents whether an observation belongs to a particular category (1) or not (0). One-hot encoding increases the dimensionality of the data, but it ensures that the GLM can capture the effects of each category independently.\n",
    "\n",
    "Example:\n",
    "For the \"Color\" categorical variable, one-hot encoding would create three separate binary variables: \"Red,\" \"Green,\" and \"Blue.\" If an observation has the category \"Red,\" the \"Red\" variable will have a value of 1, while the \"Green\" and \"Blue\" variables will be 0.\n",
    "\n",
    "It is important to note that the choice of encoding technique depends on the specific problem, the number of categories within the variable, and the desired interpretation of the coefficients. Additionally, in cases where there are a large number of categories, other techniques like entity embedding or feature hashing may be considered.\n",
    "\n",
    "By appropriately encoding categorical variables, the GLM can effectively incorporate them into the model, estimate the corresponding coefficients, and capture the relationships between the categories and the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda7866",
   "metadata": {},
   "source": [
    "7. \n",
    "The design matrix, also known as the model matrix or feature matrix, is a crucial component of the General Linear Model (GLM). It is a structured representation of the independent variables in the GLM, organized in a matrix format. The design matrix serves the purpose of encoding the relationships between the independent variables and the dependent variable, allowing the GLM to estimate the coefficients and make predictions. Here's the purpose of the design matrix in the GLM:\n",
    "\n",
    "    1. Encoding Independent Variables:\n",
    "    The design matrix represents the independent variables in a structured manner. Each column of the matrix corresponds to a specific independent variable, and each row corresponds to an observation or data point. The design matrix encodes the values of the independent variables for each observation, allowing the GLM to incorporate them into the model.\n",
    "\n",
    "    2. Incorporating Nonlinear Relationships:\n",
    "    The design matrix can include transformations or interactions of the original independent variables to capture nonlinear relationships between the predictors and the dependent variable. For example, polynomial terms, logarithmic transformations, or interaction terms can be included in the design matrix to account for nonlinearities or interactions in the GLM.\n",
    "\n",
    "    3. Handling Categorical Variables:\n",
    "    Categorical variables need to be properly encoded to be included in the GLM. The design matrix can handle categorical variables by using dummy coding or other encoding schemes. Dummy variables are binary variables representing the categories of the original variable. By encoding categorical variables appropriately in the design matrix, the GLM can incorporate them in the model and estimate the corresponding coefficients.\n",
    "\n",
    "    4. Estimating Coefficients:\n",
    "    The design matrix allows the GLM to estimate the coefficients for each independent variable. By incorporating the design matrix into the GLM's estimation procedure, the model determines the relationship between the independent variables and the dependent variable, estimating the magnitude and significance of the effects of each predictor.\n",
    "\n",
    "    5. Making Predictions:\n",
    "    Once the GLM estimates the coefficients, the design matrix is used to make predictions for new, unseen data points. By multiplying the design matrix of the new data with the estimated coefficients, the GLM can generate predictions for the dependent variable based on the values of the independent variables.\n",
    "\n",
    "Here's an example to illustrate the purpose of the design matrix:\n",
    "\n",
    "Suppose we have a GLM with a continuous dependent variable (Y) and two independent variables (X1 and X2). The design matrix would have three columns: one for the intercept (usually a column of ones), one for X1, and one for X2. Each row in the design matrix represents an observation, and the values in the corresponding columns represent the values of X1 and X2 for that observation. The design matrix allows the GLM to estimate the coefficients for X1 and X2, capturing the relationship between the independent variables and the dependent variable.\n",
    "\n",
    "In summary, the design matrix plays a crucial role in the GLM by encoding the independent variables, enabling the estimation of coefficients, and facilitating predictions. It provides a structured representation of the independent variables that can handle nonlinearities, interactions, and categorical variables, allowing the GLM to capture the relationships between the predictors and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9861422",
   "metadata": {},
   "source": [
    "8.\n",
    "Wald tests\n",
    "\n",
    "Now that we know what the coefficients mean, we can ask whether they matter. The parallel to the t\n",
    "-tests we used to do in MLR (testing whether individual coefficients were zero, and thus whether individual predictors mattered) is called a Wald test. Like all hypothesis tests, it relies on knowing the distribution of a particular test statistic if the null hypothesis is true. Here, our null hypothesis is still “this predictor isn’t useful” – that is, H0:βj=0\n",
    "\n",
    ".What is that squiggly I thing? That’s the symbol for something called the Fisher information. This is also a fun adventure for later in life. You can just think of it as a quantity that’s related to how much information you have about β, which is why the inverse of the Fisher information corresponds to the variance of ^β\n",
    "\n",
    ". This is similar to a principle we’ve seen before: the more information you have about the coefficient you’re trying to estimate, the more reliable (and therefore less variable) your estimates will be!\n",
    "\n",
    "The first thing to note here is that GLMs are fit using (dun dun dunnnnnn) maximum likelihood. Yes, your old friend ML! The coefficient estimates that you see in the R output are maximum likelihood estimates of the corresponding β’s. Now, via some fun asymptotic theory that you can enjoy later in life, you can prove what happens to the distribution of these maximum likelihood estimates as you increase the sample size:\n",
    "\n",
    "^βMLE∼N(β,I−1(^βMLE))as n→∞\n",
    "\n",
    "The immediate takeaway here is that as n→∞, your ML estimates of the β’s are unbiased, with a normal distribution.\n",
    "\n",
    "Yay! This means that we know:the SE of our estimates. This allows us to do confidence intervals for the β’s!\n",
    "the null distribution of our estimates (i.e., normal). This allows us to compare our observed values to the appropriate distribution and thus derive a p-value!\n",
    "\n",
    "\n",
    "Likelihood Ratio Tests\n",
    "Back in linear regression, we also had F tests that we used to assess the usefulness of a whole bunch of predictors at once. When we had two nested models, where the larger (full) model used all of the predictors in the smaller (reduced) model plus some extras, we could do an F test with the null hypothesis that the two models were effectively the same – the extra terms in the larger model didn’t really help it do any better at predicting the response.…and yes, I do mean that likelihood, the one we’ve seen before. It just keeps popping up! There’s a parallel test to this for logistic regression, as well. It’s called a likelihood ratio test. \n",
    "\n",
    "Like all our hypothesis tests, the LR test relies on calculating a test statistic. In this case, the test statistic is the deviance:\n",
    "\n",
    "D=2log(Lfull(b)Lreduced(b))=2(ℓfull(b)−ℓreduced(b))\n",
    "Here, Lfull(b) is the likelihood of the coefficient estimates under the larger, full model (given the observed data), and ℓfull(b) is the corresponding log-likelihood from the full model. Similarly, Lreduced(b) is the likelihood according to the smaller, reduced model, and ℓreduced(b) is the log-likelihood.\n",
    "\n",
    "Remember, the likelihood is a measure of how well the coefficients fit with the observed data. If the full model is doing a better job than the reduced model, then its coefficients should be a better fit with the data. In that case, Lfull>Lreduced, and the deviance is positive. But if the two models are equally good (more precisely, if the likelihood is the same according to both models), the deviance is 0. This test statistic is compared to a chi-squared distribution with df=dffull−dfreduced, for reasons we will not go into here :) But the interpretation of this test is quite familiar – it’s a lot like the old partial F test!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da3e05",
   "metadata": {},
   "source": [
    "9. The choice between Type II and Type III sums of squares in ANOVA and ANOVA-like models is a pretty obscure topic, but potentially important. I’m a little surprised that I only devote one page to it in Serious Stats (but that’s maybe a good thing). What’s the issue? The question arises when one has an ANOVA like model involving main effects of factors and their interactions. These models are all about partitioning variance into difference sources.\n",
    "\n",
    "If a source of variation associated with an effect is large relative to an estimate of the expected variation in a model with no effect (i.e., relative to the appropriate estimate of error variance) then we are likely to conclude that there is an effect. For this to work nicely the variances have to be cleanly partitioned. This is almost trivial in a balanced design with no covariates because all the effects are independent (assuming you have parameterised the model in an ANOVA-like way – for example through effect coding). However, if you have an unbalanced design the effects the sums of squares (SS) could be partitioned in more than one way.\n",
    "\n",
    "The main options are sequential (Type I), hierarchical (Type II) and unique (Type III) SS. Sequential SS is in arguably the most fundamental approach and preferred by purists because it involves deciding what statistical question you want to address and entering terms in sequence and partitioning according to the difference in SS explained by adding the effect to the existing model. This is approach advocated by Nelder (e.g., Nelder and Lane, 1997). The main draw back is that it fails as a useful default practice (and hence as a default for software). In addition, you can reproduce the behaviour of both hierarchical and unique SS through sequential SS if you wish.\n",
    "\n",
    "Hierarchical (Type II) involves comparing the change in SS to a model with all other effects of equal or lower order (e.g., three-way interactions, two-way interactions and main effects). Unique SS (Type III) compares SS with a model containing all other effects (regardless of order). The two are therefore equivalent in a model with no interactions or if it is completely balanced. However, they lead to potentially different outcomes if you test a main effect in a model with interactions (or a k-1 -way interactions in a model with k -way interactions). Imagine a model with two factors: A and B. Do I test the effect of A against a model with B but not AxB or a model with B and AxB? This is a source of surprising controversy and arouses strong emotions among some statisticians.\n",
    "\n",
    "If you only ever have balanced designs (or indeed near balanced designs) or don’t test interactions you don’t really need to worry about this too much (and you can probably stop reading now). However, every now and again it will matter and it is useful to consider what the best approach is.\n",
    "Advertisements\n",
    "Report this ad\n",
    "\n",
    "The fundamental source of the controversy (or at least the passions roused by it) is probably the decision to implement unique (Type III) SS as the default in SAS and SPSS (and probably other software, but SPSS seem to have copied SAS thereby making this solution the ‘correct’ solution for a whole generation of scientists educated in the heyday of SPSS).\n",
    "\n",
    "The main criticism of unique (Type III) SS is that it doesn’t respect the marginality principle. This is the principle that you can’t interpret higher order effects in models without the corresponding lower order effects: a model of the form Y ~ 1 + A + AxB is arguably inherently meaningless. Nelder and Lane write: “Neglect of marginality relations leads to the introduction of hypotheses that, although well defined mathemati- cally, are, we assert, of no inferential interest.” (This is one of the politer things that have been written about Type III SS by Nelder and others).\n",
    "\n",
    "What about the practicalities? In Serious Stats I cited Langsrud (2003) and mentioned in passing that hierarchical (Type II) SS tends to have greater statistical power. However, I have read claims that unique (Type III) SS has greater power (though I have lost the reference). This issue is examined in further detail in a very accessible paper by Smith and Cribbie (2014). Generally hierarchical (Type II) SS has greater statistical power where you most want to test the main effects and therefore is the most appropriate default:\n",
    "\n",
    "If there is no evidence of an interaction, either by way of significant hypothesis tests or effect sizes […] one of three eventualities has unfolded: (1) no interaction was detected because none exists in the population in question. In this circumstance the Type II method is definitively more powerful and we will necessarily lose power by electing to use the Type III method instead. (2) A very small interaction exists in the population, in which case it is not definitive which method will provide for the best statistical power for main effects. (3) A large interaction exists in the population but we have been extremely unfortunate in selecting a sample that does not evidence it. In this case the Type III method may hold better statistical power, but in this unfortunate situation the main effects will be of dubious value anyway. As Stewart-Oaten (1995, p.2007) quipped “the Type III SS is ‘obviously’ best for main effects only when it makes little sense to test main effects at all”. \n",
    "\n",
    "While this discussion focused on SS in ANOVA models the same considerations arise in generalized linear and related models that have an aNOVA-like design. Here we are generally interested in partitioning deviance in the model (-2logL). The marginality principle still applies here and one should adopt hierarchical (Type II) SS as the default. For R users this is probably easiest to do using the drop1() function which implements the marginality principle or (for models that take a long time to refit) using anova() to compare the two models of interest for each effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2369a86",
   "metadata": {},
   "source": [
    "10. Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It is defined as the difference between the Saturated and Proposed Models and can be thought as how much variation in the data does our Proposed Model account for. Therefore, the lower the deviance, the better the model.\n",
    "\n",
    "The deviance of a proposed model is two times the difference in maximum log-likelihoods of the saturated model and the proposed model. It’s useful for assessing the fit of a model.\n",
    "\n",
    "Some folks will be familiar with the concept of a loss function.\n",
    "Heavily glossing over the particulars, a loss function tells you how well a particular model fits to some data. If you have a lower loss, you have a better model. Finding the lowest loss (and therefore, the best model) is called optimization.\n",
    "\n",
    "In classical statistics, models are fit by maximum likelihood estimation.\n",
    "You assume some statistical model, construct a likelihood function using the statistical properties of that model, then maximize that likelihood across the parameter space. This is effectively the same strategy as optimizing a loss function, except with maximization instead of minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8b6e9",
   "metadata": {},
   "source": [
    "#### Regression:\n",
    "\n",
    "    11. What is regression analysis and what is its purpose?\n",
    "    12. What is the difference between simple linear regression and multiple linear regression?\n",
    "    13. How do you interpret the R-squared value in regression?\n",
    "    14. What is the difference between correlation and regression?\n",
    "    15. What is the difference between the coefficients and the intercept in regression?\n",
    "    16. How do you handle outliers in regression analysis?\n",
    "    17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "    18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "    19. How do you handle multicollinearity in regression analysis?\n",
    "    20. What is polynomial regression and when is it used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433782c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bde84b59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8276724e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f0560ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1938f384",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c10eee8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a0527b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c51d6330",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7537916",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f8cf2c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5bde3a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b07c8712",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a03b4270",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444091b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e9325c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3da4fa5d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
